{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5dd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m ipykernel install --user --name=<my_env_name>\n",
    "\n",
    "# https://www.tensorflow.org/tutorials/generative/dcgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c3bcb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188c86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04c7783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c14823",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a770b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705d2bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-23 10:09:46.202025: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086c9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c6452f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f81aedb6770>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46a7305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad370098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.00364857]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7faf6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2edbfdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bf8c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b366eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10ef5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75535b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fec53c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51ffac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8bf2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a62d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2514572",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a12403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e27ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tutorial has shown the complete code necessary to write and train a GAN. \n",
    "# As a next step, you might like to experiment with a different dataset, for example \n",
    "# the Large-scale Celeb Faces Attributes (CelebA) dataset available on Kaggle. To learn \n",
    "# more about GANs see the NIPS 2016 Tutorial: Generative Adversarial Networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan_sandbox",
   "language": "python",
   "name": "gan_sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
